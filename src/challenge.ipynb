{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Data Engineer - LATAM - 2024\n",
    "\n",
    "Postulante: Diego Zimmerman\n",
    "\n",
    "Email: dzimmerman2611@gmail.com\n",
    "\n",
    "Empresa: Option\n",
    "\n",
    "# Objetivo del challenge\n",
    "\n",
    "    Se me proporcion√≥ un archivo JSON (newline-delimited JSON) llamado \"farmers-protest-tweets-2021-2-4\". Este archivo contiene aproximadamente 117mil registros. Cada uno de estos registros es un objeto proveniente de la API de Twiter con informaci√≥n acerca del tweet.\n",
    "\n",
    "    Frente a esto se me pidi√≥ responder a tres preguntas:\n",
    "\n",
    "        1. Las top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as.\n",
    "        2. Los top 10 emojis m√°s usados con su respectivo conteo.\n",
    "        3. El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos.\r\n",
    "    Para cada una de estas consignas se pidi√≥ que se implementaran dos soluciones, una enfocada en la optimizaci√≥n del tiempo de ejecuci√≥n y la otra enfocada en la utilizaci√≥n de memoria.\n",
    "\n",
    "## Comentarios adicionales\n",
    "\n",
    "    Adem√°s del archivo JSONL se entreg√≥ una peque√±a estructura de proyecto con algunos archivos. Entre esos archivos (los cuales se encuentran en la carpeta `src/` del proyecto) ya estaban pre-definidas las funciones que se deb√≠an completar para responder cada una de las preguntas anteriores (en sus dos versiones).\n",
    "\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo del Challenge\n",
    "\n",
    "A continuaci√≥n voy a explicar cada uno de los √≠tems y la forma en que lo decid√≠ resolver. Si bien esta Notebook servir√° para entender el enfoque tomado, el c√≥digo se encuentra documentado y las mismas justificaciones que dar√© a continuaci√≥n se encuentran en cada una de las funciones y/o modulos implementados. Podr√°n existir detalles que no est√©n en ambos lados pero los √≠tems principales si lo est√°n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que hice fue revisar el archivo JSON para poder entender c√≥mo estaba compuesto el mismo. Aqu√≠ prest√© atenci√≥n a los pares clave-valor de cada JSON. \n",
    "\n",
    "Luego ingres√© a la documentaci√≥n oficial de la API de Twitter para contrastar lo que estaba observando con lo que dec√≠a la documentaci√≥n.\n",
    "Esto lo hice para poder asegurar que la estructura de los JSONs ser√≠an constantes y poder evaluar mejor qu√© m√©todo de lectura ser√≠a mejor para este tipo de archivo.\n",
    "\n",
    "Luego comenc√© a pensar en cada funci√≥n en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports y variables comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<app.logger.Logger at 0x7facd6943910>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.constants import DATA_DIR\n",
    "from src.q1_memory import q1_memory\n",
    "from src.q1_time import q1_time\n",
    "from src.q2_memory import q2_memory\n",
    "from src.q2_time import q2_time\n",
    "from src.q3_memory import q3_memory\n",
    "from src.q3_time import q3_time\n",
    "from app.main import Logger\n",
    "\n",
    "Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = DATA_DIR / \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Top 10 fechas donde hay m√°s tweets. Mencionar el usuario (username) que m√°s publicaciones tiene por cada uno de esos d√≠as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el uso de memoria, esta funci√≥n itera el archivo JSONL l√≠nea por l√≠nea. Procesar l√≠nea por l√≠nea implica procesar un JSON a la vez.\n",
    "\n",
    "El objetivo de esto es evitar cargar en memoria la totalidad del archivo y en cambio procesar cada JSON por separado. Una vez que se termin√≥ de procesar un JSON, guardar lo que sea necesario en memoria y descartar lo dem√°s. Reci√©n ah√≠ continuar con la siguiente l√≠nea del archivo (es decir, el siguiente JSON).\n",
    "\n",
    "**Explicaci√≥n de funci√≥n**\n",
    "\n",
    "En esta funci√≥n decid√≠ hacer una doble lectura del archivo con la intenci√≥n de optimizar a√∫n m√°s el uso de la memoria. Para ello describir√© los dos enfoques posibles:\n",
    "1. Se puede leer una √∫nica ver el archivo (leyendo l√≠nea por l√≠nea) y que a la misma vez que se est√° contando la cantidad de tweets por d√≠a se est√© contando la cantidad de tweets que cada usuario hizo para los distintos d√≠as.\n",
    "2. Se puede leer una primera vez el archivo (leyendo l√≠nea por l√≠nea) y contar la cantidad de tweets para cada fecha. Una vez finalizada esta cuenta, encontrar los 10 d√≠as con m√°s tweets. Una vez finalizada esta b√∫squeda, leer√© por segunda vez el archivo (leyendo l√≠nea por l√≠nea) a fin de contar la cantidad de tweets de cada usuario. La diferencia es que ahora solo contar√© los tweets de usuarios que solo hayan publicado tweets en las fechas que salieron dentro del TOP 10.\n",
    "\n",
    "Como se puede observar, el segundo enfoque permite que guarde en memoria la cuenta de tweets de usuarios que efectivamente publicaron algo en los d√≠as que salieron en el TOP 10 y no tener que almacenar la cuenta para todos los dem√°s d√≠as que no son de inter√©s en esta respuesta.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. Se evitaron variables intermedias y se prioriz√≥ anidar calculos a fin de evitar utilizar espacios de memorias para almacenar estas variables intermedias.\n",
    "2. Se decidi√≥ utilizar el m√©todo `nlargest` de la l√≠brer√≠a `heapq` frente a otros enfoques como `sorted()` dado que este m√©todo evita tener que cargar el listado completo de elementos en memor√≠a para reci√©n poder ordenarlos. El m√©todo utilizado permite ir guardando en memoria solo una cantidad N de elementos (en este caso 10) mientras se va iterando sobre el resto del listado (cargando un elemento a la vez en memoria).\n",
    "3. Se hace uso de `del` para eliminar variables intermedias que eran necesarias pero que despu√©s dejan de serlo (ya que su prop√≥sito fue cumplido) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la funci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_memory(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora le haremos el perfilado de tiempo y de consumo de memoria para despu√©s poder comparar con el otro enfoque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         11343289 function calls (11342043 primitive calls) in 221.810 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    17     48.5 MiB     48.5 MiB           1   @profile_function\n",
    "    18                                         @memory_profile_logging_wrapper\n",
    "    19                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    20                                             \"\"\"Answer question 1 efficiently in memory.\n",
    "    21                                         \n",
    "    22                                                 Objective: Find the 10 dates with the most tweets and\n",
    "    23                                                 the most active user for each date.\n",
    "    24                                         \n",
    "    25                                                 This function iterates over a json file and gets the TOP 10\n",
    "    26                                                 dates on which users have tweeted the most.\n",
    "    27                                         \n",
    "    28                                                 Once this is done, this function iterates over the same json looking\n",
    "    29                                                 for the most active user for each of these 10 dates.\n",
    "    30                                         \n",
    "    31                                                 We can find the users for each date in a faster way\n",
    "    32                                                 and reading the json only once, but we are interested\n",
    "    33                                                 in memory efficiency. That is why we want to avoid having a\n",
    "    34                                                 dictionary with the users' activity until we know which\n",
    "    35                                                 dates we are interested in.\n",
    "    36                                         \n",
    "    37                                             Parameters\n",
    "    38                                             ----------\n",
    "    39                                             file_path : str\n",
    "    40                                                 Path of the json file to be loaded\n",
    "    41                                         \n",
    "    42                                             Returns\n",
    "    43                                             -------\n",
    "    44                                             List[Tuple[datetime.date, str]]\n",
    "    45                                                 List of the 10 most popular dates and their users.\n",
    "    46                                                 The date and user are returned as a tuple.\n",
    "    47                                                 The list is sorted in descending order\n",
    "    48                                                 (the most tweeted date will appear first).\n",
    "    49                                             \"\"\"\n",
    "    50     48.5 MiB      0.0 MiB           1       module_logger.info(\"Starting Q1-MEMORY...\")\n",
    "    51                                         \n",
    "    52     48.5 MiB      0.0 MiB           1       module_logger.info(\"Finding TOP 10 dates\")\n",
    "    53                                             # Initialize dict that will store the number of tweets per day.\n",
    "    54     48.5 MiB      0.0 MiB           1       tweet_count_by_date = defaultdict(int)\n",
    "    55                                             # Load json line by line so memory usage is optimized\n",
    "    56     49.0 MiB      0.0 MiB           2       with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    57     49.0 MiB      0.3 MiB      117408           for line in file:\n",
    "    58     49.0 MiB      0.0 MiB      117407               tweet = json.loads(line)\n",
    "    59                                                     # Extract date and parse it\n",
    "    60                                                     # Increment tweet counter by date\n",
    "    61     49.0 MiB      0.0 MiB      352221               tweet_count_by_date[\n",
    "    62     49.0 MiB      0.3 MiB      117407                   datetime.strptime(tweet['date'].split('T')[0], DATE_FORMAT).date()\n",
    "    63     49.0 MiB      0.0 MiB      117407               ] += 1\n",
    "    64                                         \n",
    "    65                                             # Find TOP 10 dates\n",
    "    66                                             # We use heapq instead of sorted:\n",
    "    67                                             #   The heap method only stores the top n elements at any time,\n",
    "    68                                             #   whereas the sorting method requires storage of all N elements.\n",
    "    69                                             #   This means that for large datasets, the heap method's\n",
    "    70                                             #   memory usage remains constant\n",
    "    71     49.0 MiB      0.0 MiB           2       top_10_dates = heapq.nlargest(\n",
    "    72     49.0 MiB      0.0 MiB           1           10,\n",
    "    73     49.0 MiB      0.0 MiB           1           tweet_count_by_date.keys(),\n",
    "    74     49.0 MiB      0.0 MiB          27           key=lambda date: tweet_count_by_date[date],  # noqa: F821\n",
    "    75                                             )\n",
    "    76     49.0 MiB      0.0 MiB           1       del tweet_count_by_date\n",
    "    77     49.0 MiB      0.0 MiB           1       module_logger.info(\"TOP 10 dates found.\")\n",
    "    78                                         \n",
    "    79     49.0 MiB      0.0 MiB           1       module_logger.info(\"Finding most active user for each date\")\n",
    "    80                                             # Initialize dict that will store the most active user.\n",
    "    81     49.0 MiB      0.0 MiB           1       result = []\n",
    "    82     52.4 MiB      0.0 MiB          21       user_tweet_count_by_date = defaultdict(lambda: defaultdict(int))\n",
    "    83                                             # Load json line by line so memory usage is optimized\n",
    "    84     52.0 MiB      0.0 MiB           2       with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    85     52.9 MiB      0.0 MiB      117408           for line in file:\n",
    "    86     52.9 MiB      2.8 MiB      117407               tweet = json.loads(line)\n",
    "    87                                         \n",
    "    88                                                     # If date is not in `top_10_dates` skip iteration\n",
    "    89                                                     # If the top_10_dates was larger (more than 10 dates)\n",
    "    90                                                     # we should think of another data type to store these dates.\n",
    "    91                                                     # to make this `not in` clause more memory efficient.\n",
    "    92                                                     # Given that we only have 10 dates, we are going to consider\n",
    "    93                                                     # this improvement as unnecessary for now.\n",
    "    94                                                     if (\n",
    "    95     52.9 MiB      0.0 MiB      234814                   datetime.strptime(tweet['date'].split('T')[0], DATE_FORMAT).date()\n",
    "    96     52.9 MiB      0.0 MiB      117407                   not in top_10_dates\n",
    "    97                                                     ):\n",
    "    98     49.3 MiB      0.0 MiB       18040                   continue\n",
    "    99                                         \n",
    "   100     52.9 MiB      1.0 MiB      397468               user_tweet_count_by_date[\n",
    "   101     52.9 MiB      0.0 MiB       99367                   datetime.strptime(tweet['date'].split('T')[0], DATE_FORMAT).date()\n",
    "   102     52.9 MiB      0.0 MiB      198734               ][tweet[\"user\"]['username']] += 1\n",
    "   103                                         \n",
    "   104     52.9 MiB      0.0 MiB          11           for top_date in top_10_dates:\n",
    "   105                                                     # Find TOP 10 dates\n",
    "   106                                                     # We use heapq instead of sorted:\n",
    "   107                                                     #   The heap method only stores the top n elements at any time,\n",
    "   108                                                     #   whereas the sorting method requires storage of all N elements.\n",
    "   109                                                     #   This means that for large datasets, the heap method's\n",
    "   110                                                     #   memory usage remains constant\n",
    "   111                                                     # pylint: disable=W0640\n",
    "   112     52.9 MiB      0.0 MiB          30               top_user = heapq.nlargest(\n",
    "   113     52.9 MiB      0.0 MiB          10                   1,\n",
    "   114     52.9 MiB      0.0 MiB          10                   user_tweet_count_by_date[top_date].keys(),\n",
    "   115     52.9 MiB      0.0 MiB      132487                   key=lambda username: user_tweet_count_by_date[top_date][  # noqa: F821\n",
    "   116     52.9 MiB      0.0 MiB       44159                       username\n",
    "   117                                                         ],\n",
    "   118     52.9 MiB      0.0 MiB          10               )[0]\n",
    "   119                                                     # pylint: enable=W0640\n",
    "   120     52.9 MiB      0.0 MiB          10               result.append((top_date, top_user))\n",
    "   121     52.0 MiB     -0.9 MiB           1           del user_tweet_count_by_date\n",
    "   122     52.0 MiB      0.0 MiB           1       module_logger.info(\"Most active user for each date found\")\n",
    "   123     52.0 MiB      0.0 MiB           1       module_logger.info(\"Finishing Q1-MEMORY...\")\n",
    "   124     52.0 MiB      0.0 MiB           1       return resultodule_logger.info(\"Finishing Q1-MEMORY...\")\r\n",
    "   124    671.8 MiB      0.0 MiB           1       return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el tiempo, esta funci√≥n busca trabajar con todo el archivo JSONL de una sola vez, evitando tener que procesar el mismo l√≠nea por l√≠nea. Si bien esto tiene un efecto significativo en memoria, es importante destacar que aqu√≠ asum√≠ que este c√≥digo siempre se ejecutar√° en una PC con capacidad de c√≥mputo y memoria suficiente.\n",
    "\n",
    "**Explicaci√≥n de funci√≥n**\n",
    "\n",
    "Si bien la lectura l√≠nea por l√≠nea tambi√©n fue necesaria aqu√≠ (dada la estructura JSONL), en esta oportunidad decid√≠ cargar en memoria todas las l√≠neas del archivo antes de empezar a procesar los datos.\n",
    "\n",
    "De esta manera, cargu√© todos los JSONs dentro del archivo en una lista que luego convert√≠ en un DataFrame de `polars`. Una vez hecho esto realic√© operaciones sencillas de agrupaci√≥n (group by) y agregaci√≥n (count) para llegar a la respuesta deseada.\n",
    "\r",
    "**NOTAS SOBRE EL CODIGO**\r",
    "1. a.\r\n",
    "Utilizo la librer√≠a orjson por lo que se detalla en la secci√≥n \"Observaciones generales del Challenge\" m√°s abajo en esta notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la funci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cProfile results:\n",
    "         339557 function calls (339456 primitive calls) in 11.299 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    17     52.0 MiB     52.0 MiB           1   @profile_function\n",
    "    18                                         @memory_profile_logging_wrapper\n",
    "    19                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    20                                             \"\"\"Answer question 1 efficiently in time.\n",
    "    21                                         \n",
    "    22                                                 In this case I've used the `polars` library instead of\n",
    "    23                                                 pandas because of the results that can be seen in the\n",
    "    24                                                 `notebooks/Q1 processing time.ipynb` nootebook.\n",
    "    25                                         \n",
    "    26                                                 There is a small advantage for polars so I decided to keep\n",
    "    27                                                 polars.\n",
    "    28                                         \n",
    "    29                                                 Note: Given the size of the JSON file, there is no need for\n",
    "    30                                                 parallel processing so PySpark and Dask were discarded. These\n",
    "    31                                                 two options could had beeen an excellent solution\n",
    "    32                                                 if the size of the size happened to be bigger.\n",
    "    33                                         \n",
    "    34                                             Parameters\n",
    "    35                                             ----------\n",
    "    36                                             file_path : str\n",
    "    37                                                 Path of the json file to be loaded\n",
    "    38                                         \n",
    "    39                                             Returns\n",
    "    40                                             -------\n",
    "    41                                             List[Tuple[datetime.date, str]]\n",
    "    42                                                 List of the 10 most popular dates and their users.\n",
    "    43                                                 The date and user are returned as a tuple.\n",
    "    44                                                 The list is sorted in descending order\n",
    "    45                                                 (the most tweeted date will appear first).\n",
    "    46                                             \"\"\"\n",
    "    47     52.0 MiB      0.0 MiB           1       module_logger.info(\"Starting Q1-TIME...\")\n",
    "    48                                         \n",
    "    49     52.0 MiB      0.0 MiB           1       module_logger.info(\"Reading JSON file.\")\n",
    "    50     74.0 MiB     21.9 MiB           1       data = read_json_file(file_path)\n",
    "    51     74.0 MiB      0.0 MiB           1       module_logger.info(\"JSON file is read.\")\n",
    "    52                                         \n",
    "    53     74.0 MiB      0.0 MiB           1       module_logger.info(\"Loading DataFrame\")\n",
    "    54                                             # Create a Polars DataFrame from the processed data\n",
    "    55    100.1 MiB     26.1 MiB           2       tweets_df: pl.DataFrame = pl.DataFrame(\n",
    "    56     74.0 MiB      0.0 MiB           1           data, schema=[\"date\", \"username\"], orient=\"row\"\n",
    "    57                                             )\n",
    "    58                                         \n",
    "    59    100.1 MiB      0.0 MiB           1       module_logger.info(\"Parsing date column\")\n",
    "    60                                             # Convert the 'date' column to datetime format\n",
    "    61    154.9 MiB     54.8 MiB           2       tweets_df = tweets_df.with_columns(\n",
    "    62    100.1 MiB      0.0 MiB           1           [pl.col(\"date\").str.strptime(pl.Datetime, format=DATETIME_FORMAT, strict=False)]\n",
    "    63                                             )\n",
    "    64                                         \n",
    "    65    154.9 MiB      0.0 MiB           1       module_logger.info(\"Finding top users by date\")\n",
    "    66                                             # Group by date and username, counting the number of tweets per user each day\n",
    "    67    164.8 MiB     10.0 MiB           2       tweets_per_day = tweets_df.group_by([pl.col(\"date\").dt.date(), \"username\"]).agg(\n",
    "    68    154.9 MiB      0.0 MiB           1           pl.count().alias(\"tweet_count\")\n",
    "    69                                             )\n",
    "    70                                         \n",
    "    71                                             # Find the user with the most tweets per day\n",
    "    72    165.1 MiB      0.0 MiB           1       top_user_per_day = (\n",
    "    73    164.8 MiB      0.0 MiB           1           tweets_per_day.sort(\"tweet_count\", descending=True)\n",
    "    74    164.8 MiB      0.0 MiB           1           .group_by(\"date\")\n",
    "    75    165.1 MiB      0.3 MiB           1           .agg(pl.first(\"username\").alias(\"top_user\"))\n",
    "    76                                             )\n",
    "    77                                         \n",
    "    78    165.1 MiB      0.0 MiB           1       module_logger.info(\"Finding most tweeted dates\")\n",
    "    79                                             # Count the total number of tweets per day\n",
    "    80    165.4 MiB      0.2 MiB           2       tweets_by_day = tweets_df.group_by(tweets_df[\"date\"].dt.date()).agg(\n",
    "    81    165.1 MiB      0.0 MiB           1           pl.count().alias(\"total_tweets\")\n",
    "    82                                             )\n",
    "    83                                         \n",
    "    84    165.4 MiB      0.0 MiB           1       module_logger.info(\"Joining top users with top dates\")\n",
    "    85                                             # Join the total tweets with the top user by day\n",
    "    86    167.5 MiB      2.1 MiB           1       top_10_dates = tweets_by_day.join(top_user_per_day, on=\"date\")\n",
    "    87                                         \n",
    "    88    167.5 MiB      0.0 MiB           1       module_logger.info(\"Parsing output\")\n",
    "    89                                             # Sort by total number of tweets per day and select the top 10 dates\n",
    "    90    167.5 MiB      0.0 MiB           1       top_10_dates = top_10_dates.sort(\"total_tweets\", descending=True).head(10)\n",
    "    91                                         \n",
    "    92    167.5 MiB      0.0 MiB          24       return [\n",
    "    93    167.5 MiB      0.0 MiB          11           (row[\"date\"], row[\"top_user\"]) for row in top_10_dates.iter_rows(named=True)\n",
    "    94                                             ]tes.iter_rows(named=True)\r\n",
    "    94                                             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Los top 10 emojis m√°s usados con su respectivo conteo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el uso de memoria, esta funci√≥n itera el archivo JSONL l√≠nea por l√≠nea. Procesar l√≠nea por l√≠nea implica procesar un JSON a la vez.\n",
    "\n",
    "El objetivo de esto es evitar cargar en memoria la totalidad del archivo y en cambio procesar cada JSON por separado. Una vez que se termin√≥ de procesar un JSON, guardar lo que sea necesario en memoria y descartar lo dem√°s. Reci√©n ah√≠ continuar con la siguiente l√≠nea del archivo (es decir, el siguiente JSON).\n",
    "\n",
    "**Explicaci√≥n de funci√≥n**\n",
    "\n",
    "Para poder encontrar los emojis decid√≠ hacer uso de expresiones regulares (RegEx). Para esto defin√≠ la constante `EMOJI_PATTERN` dentro del m√≥dulo `app.constants` que sirve para encontrar los emojis dentro de esos valores pre-definidos.\n",
    "\n",
    "Al cargar l√≠nea por l√≠nea el archivo, hice uso de esta constante que ya tiene definido el pattern para la expresi√≥n regular e hice uso del m√©todo `findall()`. Dado que los tweets son textos medianamente corto, el utilizar `findall()` resulta una estrategia acertada. Si el texto fuese m√°s largo, deber√≠a evaluarse el uso de `finditer()` el cual puede ayudar a alivianar la carga en memoria en caso de que se encontrasen muchos emojis en un √∫nico tweet.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. Se evitaron variables intermedias y se prioriz√≥ anidar calculos a fin de evitar utilizar espacios de memorias para almacenar estas variables intermedias.\n",
    "2. Se decidi√≥ utilizar el m√©todo `nlargest` de la l√≠brer√≠a `heapq` frente a otros enfoques como `sorted()` dado que este m√©todo evita tener que cargar el listado completo de elementos en memor√≠a para reci√©n poder ordenarlos. El m√©todo utilizado permite ir guardando en memoria solo una cantidad N de elementos (en este caso 10) mientras se va iterando sobre el resto del listado (cargando un elemento a la vez en memoria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la funci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üôè', 1940),\n",
       " ('‚ù§', 1397),\n",
       " ('üåæ', 523),\n",
       " ('üíö', 492),\n",
       " ('üòÇ', 488),\n",
       " ('üëç', 458),\n",
       " ('üëâ', 450),\n",
       " ('‚úä', 425),\n",
       " ('üáÆüá≥', 407),\n",
       " ('üôèüôè', 393)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         1396769 function calls (1396766 primitive calls) in 51.556 seconds.0 MiB           1       return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    16    154.6 MiB    154.6 MiB           1   @profile_function\n",
    "    17                                         @memory_profile_logging_wrapper\n",
    "    18                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    19                                             \"\"\"Answer question 2 efficiently in memory.\n",
    "    20                                         \n",
    "    21                                                 The approach used in the function is to load the JSON\n",
    "    22                                                 file line by line.\n",
    "    23                                                 If the `content' key exists, then we look for emojis in the content.\n",
    "    24                                                 Once emojis are found for that line, the dictionary with the counts is\n",
    "    25                                                 updated.\n",
    "    26                                                 After every line has been reviewed, we look for the TOP 10.\n",
    "    27                                         \n",
    "    28                                             Parameters\n",
    "    29                                             ----------\n",
    "    30                                             file_path : str\n",
    "    31                                                 Path of the json file to be loaded\n",
    "    32                                         \n",
    "    33                                             Returns\n",
    "    34                                             -------\n",
    "    35                                             List[Tuple[datetime.date, str]]\n",
    "    36                                                 List of the 10 most used emojis and\n",
    "    37                                                 the number of times they had been used.\n",
    "    38                                             \"\"\"\n",
    "    39    154.6 MiB      0.0 MiB           1       module_logger.info(\"Starting Q2-MEMORY...\")\n",
    "    40                                         \n",
    "    41    154.6 MiB      0.0 MiB           1       module_logger.info(\"Counting emojis\")\n",
    "    42    154.6 MiB      0.0 MiB           1       emoji_counter = defaultdict(int)\n",
    "    43    154.6 MiB     -1.0 MiB           2       with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    44    154.6 MiB -36855.5 MiB      117408           for line in file:\n",
    "    45    154.6 MiB -36855.5 MiB      117407               data = json.loads(line)\n",
    "    46    154.6 MiB -36855.5 MiB      117407               if 'content' not in data:\n",
    "    47                                                         continue\n",
    "    48                                                     # Extract all emojis from tweet and update dict with count\n",
    "    49    154.6 MiB -44350.6 MiB      139888               for match in EMOJI_PATTERN.findall(data[\"content\"]):\n",
    "    50    154.6 MiB  -7495.1 MiB       22481                   if match:  # Only count non-None matches\n",
    "    51    154.6 MiB  -7495.1 MiB       22481                       emoji_counter[match] += 1\n",
    "    52    153.6 MiB     -1.0 MiB           1       module_logger.info(\"Emojis counted.\")\n",
    "    53                                         \n",
    "    54    153.6 MiB      0.0 MiB           1       module_logger.info(\"Finding TOP 10 emojis\")\n",
    "    55                                             # Top 10 emojis\n",
    "    56    153.6 MiB      0.0 MiB        6227       top_10_emojis = heapq.nlargest(10, emoji_counter.items(), key=lambda x: x[1])\n",
    "    57    153.6 MiB      0.0 MiB           1       module_logger.info(\"TOP 10 emojis found.\")\n",
    "    58    153.6 MiB      0.0 MiB           1       module_logger.info(\"Finishing Q2-MEMORY...\")\n",
    "    59    153.6 MiB      0.0 MiB           1       return top_10_emojis.0 MiB           1       return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el tiempo, esta funci√≥n busca trabajar con todo el archivo JSONL de una sola vez, evitando tener que procesar el mismo l√≠nea por l√≠nea. Si bien esto tiene un efecto significativo en memoria, es importante destacar que aqu√≠ asum√≠ que este c√≥digo siempre se ejecutar√° en una PC con capacidad de c√≥mputo y memoria suficiente.\n",
    "\n",
    "**Explicaci√≥n de funci√≥n**\n",
    "\n",
    "Si bien la lectura l√≠nea por l√≠nea tambi√©n fue necesaria aqu√≠ (dada la estructura JSONL), en esta oportunidad decid√≠ cargar en memoria todas las l√≠neas del archivo antes de empezar a procesar los datos.\n",
    "\n",
    "Como el uso de memoria no es una limitaci√≥n en este caso, lo que haremos ser√° cargar todos los tweets en un solo `string` y luego procesar este string con una Expresi√≥n Regular (RegEx) que contendr√° todos los emojis que se desean encontrar.\n",
    "\n",
    "Luego de obtener la lista de emojis totales, utilizaremos un Contador para devolver los 10 primeros de estos y su correspondiente n√∫mero de apariciones.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. En este caso decidi usar la clase `Counter` la cual resuelve de manera eficiente y sencilla las operaciones de cuenta.\n",
    "2. Utilizo la librer√≠a `orjson` por lo que se detalla en la secci√≥n \"Observaciones generales del Challenge\" m√°s abajo en esta notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la funci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('üôè', 1940),\n",
       " ('‚ù§', 1397),\n",
       " ('üåæ', 523),\n",
       " ('üíö', 492),\n",
       " ('üòÇ', 488),\n",
       " ('üëç', 458),\n",
       " ('üëâ', 450),\n",
       " ('‚úä', 425),\n",
       " ('üáÆüá≥', 407),\n",
       " ('üôèüôè', 393)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         219509 function calls (219506 primitive calls) in 39.018 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    16    153.6 MiB    153.6 MiB           1   @profile_function\n",
    "    17                                         @memory_profile_logging_wrapper\n",
    "    18                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    19                                             \"\"\"Answer question 2 efficiently in time.\n",
    "    20                                         \n",
    "    21                                                 Since memory usage is not a limitation in this case,\n",
    "    22                                                 what we will do is load all the tweets into a single string\n",
    "    23                                                 and then process it with a Regular Expression (RegEx) that will contain\n",
    "    24                                                 all the emojis existing in the `emoji` library.\n",
    "    25                                         \n",
    "    26                                                 After obtaining the list of total emojis, we will use a Counter\n",
    "    27                                                 to return the top 10 of these and their corresponding\n",
    "    28                                                 number of appearances.\n",
    "    29                                         \n",
    "    30                                             Parameters\n",
    "    31                                             ----------\n",
    "    32                                             file_path : str\n",
    "    33                                                 Path of the json file to be loaded\n",
    "    34                                         \n",
    "    35                                             Returns\n",
    "    36                                             -------\n",
    "    37                                             List[Tuple[datetime.date, str]]\n",
    "    38                                                 List of the 10 most used emojis and\n",
    "    39                                                 the number of times they had been used.\n",
    "    40                                             \"\"\"\n",
    "    41    153.6 MiB      0.0 MiB           1       module_logger.info(\"Starting Q2-TIME...\")\n",
    "    42    153.6 MiB      0.0 MiB           1       module_logger.info(\"Reading and parsing JSON file.\")\n",
    "    43    153.6 MiB      0.0 MiB           1       full_text = \"\"\n",
    "    44    637.0 MiB      0.0 MiB           2       with open(file_path, 'r', encoding=\"utf-8\") as json_file:\n",
    "    45    561.2 MiB    407.6 MiB           1           tweet_list = json_file.readlines()\n",
    "    46                                         \n",
    "    47    561.2 MiB      0.0 MiB           1           try:\n",
    "    48    637.0 MiB     53.8 MiB           2               full_text = \" \".join(\n",
    "    49    583.2 MiB      4.9 MiB      234818                   [\n",
    "    50    583.2 MiB     17.0 MiB      117407                       orjson.loads(tweet)['content']  # pylint: disable=E1101\n",
    "    51    583.2 MiB      0.0 MiB      117408                       for tweet in tweet_list\n",
    "    52                                                         ]\n",
    "    53                                                     )\n",
    "    54                                                 except orjson.JSONDecodeError:  # pylint: disable=E1101\n",
    "    55                                                     module_logger.error(\n",
    "    56                                                         \"There was an error reading the JSON. Some fields may be missing.\"\n",
    "    57                                                     )\n",
    "    58                                                     module_logger.info(\"The request couldn't be fulfilled.\")\n",
    "    59                                                     return []\n",
    "    60    637.0 MiB      0.0 MiB           1       module_logger.info(\"JSON file was read and parsed.\")\n",
    "    61                                         \n",
    "    62    637.0 MiB      0.0 MiB           1       module_logger.info(\"Finding top 10 emojis\")\n",
    "    63    637.0 MiB      0.0 MiB           1       top_10_emojis = Counter(EMOJI_PATTERN.findall(full_text)).most_common(10)\n",
    "    64                                         \n",
    "    65    637.0 MiB      0.0 MiB           1       module_logger.info(\"Finishing Q2-TIME...\")\n",
    "    66    637.0 MiB      0.0 MiB           1       return top_10_emojis      0.0 MiB           1       return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El top 10 hist√≥rico de usuarios (username) m√°s influyentes en funci√≥n del conteo de las menciones (@) que registra cada uno de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el uso de memoria, esta funci√≥n itera el archivo JSONL l√≠nea por l√≠nea. Procesar l√≠nea por l√≠nea implica procesar un JSON a la vez.\n",
    "\n",
    "El objetivo de esto es evitar cargar en memoria la totalidad del archivo y en cambio procesar cada JSON por separado. Una vez que se termin√≥ de procesar un JSON, guardar lo que sea necesario en memoria y descartar lo dem√°s. Reci√©n ah√≠ continuar con la siguiente l√≠nea del archivo (es decir, el siguiente JSON).\n",
    "\n",
    "**Explicaci√≥n de funci√≥n**\n",
    "\n",
    "Para poder encontrar los emojis decid√≠ hacer uso de expresiones regulares (RegEx). Para esto defin√≠ la constante `EMOJI_PATTERN` dentro del m√≥dulo `app.constants` que sirve para encontrar los emojis dentro de esos valores pre-definidos.\n",
    "\n",
    "Al cargar l√≠nea por l√≠nea el archivo, hice uso de esta constante que ya tiene definido el pattern para la expresi√≥n regular e hice uso del m√©todo `findall()`. Dado que los tweets son textos medianamente corto, el utilizar `findall()` resulta una estrategia acertada. Si el texto fuese m√°s largo, deber√≠a evaluarse el uso de `finditer()` el cual puede ayudar a alivianar la carga en memoria en caso de que se encontrasen muchos emojis en un √∫nico tweet.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. Se evitaron variables intermedias y se prioriz√≥ anidar calculos a fin de evitar utilizar espacios de memorias para almacenar estas variables intermedias.\n",
    "2. Se decidi√≥ utilizar el m√©todo `nlargest` de la l√≠brer√≠a `heapq` frente a otros enfoques como `sorted()` dado que este m√©todo evita tener que cargar el listado completo de elementos en memor√≠a para reci√©n poder ordenarlos. El m√©todo utilizado permite ir guardando en memoria solo una cantidad N de elementos (en este caso 10) mientras se va iterando sobre el resto del listado (cargando un elemento a la vez en memoria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la funci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         1615534 function calls (1615531 primitive calls) in 66.063 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    15    570.6 MiB    570.6 MiB           1   @profile_function\n",
    "    16                                         @memory_profile_logging_wrapper\n",
    "    17                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    18                                             \"\"\"Answer question 3 efficiently in memory.\n",
    "    19                                         \n",
    "    20                                                 In this function we had two possible paths:\n",
    "    21                                                     1. Use a RegEx to find the mentions (@) in the context/text\n",
    "    22                                                       of the tweet.\n",
    "    23                                                     2. Use the `mentionedUsers` key in the JSON and leaving to\n",
    "    24                                                       the API of Twitter the identification of those mentions\n",
    "    25                                         \n",
    "    26                                                 I decided to go with the second approach.\n",
    "    27                                         \n",
    "    28                                                 This approach also include reading the JSON file line by line\n",
    "    29                                                 and counting how many times the same user is mentioned.\n",
    "    30                                                 At the end the TOP 10 users is found.\n",
    "    31                                         \n",
    "    32                                             Parameters\n",
    "    33                                             ----------\n",
    "    34                                             file_path : str\n",
    "    35                                                 Path of the json file to be loaded\n",
    "    36                                         \n",
    "    37                                             Returns\n",
    "    38                                             -------\n",
    "    39                                             List[Tuple[datetime.date, str]]\n",
    "    40                                                 List of the 10 most mentioned users and\n",
    "    41                                                 the number of times they had been mentioned.\n",
    "    42                                             \"\"\"\n",
    "    43    570.6 MiB      0.0 MiB           1       module_logger.info(\"Starting Q3-MEMORY...\")\n",
    "    44    570.6 MiB      0.0 MiB           1       users_counter = defaultdict(int)\n",
    "    45    570.6 MiB      0.0 MiB           1       module_logger.info(\"Counting mentioned users\")\n",
    "    46    570.6 MiB     -2.0 MiB           2       with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    47    570.6 MiB -177413.6 MiB      117408           for line in file:\n",
    "    48    570.6 MiB -177413.6 MiB      117407               mentioned_users = json.loads(line).get(\"mentionedUsers\")\n",
    "    49                                         \n",
    "    50    570.6 MiB -177413.6 MiB      117407               if mentioned_users is None:\n",
    "    51    570.6 MiB -118823.9 MiB       79373                   continue\n",
    "    52                                         \n",
    "    53    570.6 MiB -214651.3 MiB      141437               for user in mentioned_users:\n",
    "    54    570.6 MiB -156061.6 MiB      103403                   if user.get(\"username\") is not None:\n",
    "    55    570.6 MiB -156061.6 MiB      103403                       users_counter[user.get(\"username\")] += 1\n",
    "    56                                         \n",
    "    57    568.6 MiB     -2.0 MiB           1       module_logger.info(\"Finding TOP 10 mentioned users\")\n",
    "    58    568.6 MiB      0.0 MiB       30479       top_10_users = heapq.nlargest(10, users_counter.items(), key=lambda x: x[1])\n",
    "    59                                         \n",
    "    60    568.6 MiB      0.0 MiB           1       module_logger.info(\"Ending Q3-MEMORY...\")\n",
    "    61                                         \n",
    "    62    568.6 MiB      0.0 MiB           1       return top_10_users    0.0 MiB           1       return top_10_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Enfoque en eficiencia de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el tiempo, esta funci√≥n busca trabajar con todo el archivo JSONL de una sola vez, evitando tener que procesar el mismo l√≠nea por l√≠nea. Si bien esto tiene un efecto significativo en memoria, es importante destacar que aqu√≠ asum√≠ que este c√≥digo siempre se ejecutar√° en una PC con capacidad de c√≥mputo y memoria suficiente.\n",
    "\n",
    "**Explicaci√≥n de funci√≥n**\n",
    "\n",
    "Si bien la lectura l√≠nea por l√≠nea tambi√©n fue necesaria aqu√≠ (dada la estructura JSONL), en esta oportunidad decid√≠ cargar en memoria todas las l√≠neas del archivo antes de empezar a procesar los datos.\n",
    "\n",
    "Lo que haremos es cargar cada JSON (cada l√≠nea del JSONL) y extraer de √©l el campo `mentionedUsers`. Este campo es a su vez una lista de diccionarios donde cada diccionario son los datos del usuario que fue mencionado en el tweet. De esta manera, al momento de la carga de JSON en memoria, extraemos de este campo `mentionedUsers` todos los `usernames` que fueron mencionados en el tweet que estamos procesando.\n",
    "\n",
    "Una vez procesada la l√≠nea en cuesti√≥n se actualiza el objeto `Counter` y se sigue procesando la siguiente l√≠nea del archivo. De esta manera, cuando se termine de cargar el archivo se tendr√° un objeto `Counter` con la cuenta que estamos buscando.\n",
    "\n",
    "Luego de obtener la lista de emojis totales, utilizaremos un Contador para devolver los 10 primeros de estos y su correspondiente n√∫mero de apariciones.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. En este caso decidi usar la clase `Counter` la cual resuelve de manera eficiente y sencilla las operaciones de cuenta.\n",
    "2. Utilizo la librer√≠a `orjson` por lo que se detalla en la secci√≥n \"Observaciones generales del Challenge\" m√°s abajo en esta notebook.\n",
    "\n",
    "**NOTA SOBRE RESULTADO**\n",
    "Se puede observar que el enfoque para la eficiencia en memoria y en tiempo son muy parecidos, por lo cual no se observan diferencia notorias entre las dos funciones. Ambos resultados son buenos en memoria como en tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la funci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         565055 function calls (565052 primitive calls) in 60.092 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    15    568.6 MiB    568.6 MiB           1   @profile_function\n",
    "    16                                         @memory_profile_logging_wrapper\n",
    "    17                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    18                                             \"\"\"Answer question 3 efficiently in time.\n",
    "    19                                         \n",
    "    20                                                 In this function, I decided to read line by line in order\n",
    "    21                                                 to extract the mentionedUsers object from each JSON.\n",
    "    22                                         \n",
    "    23                                                 Once the mentionedUser object is retrieve a simple operation with\n",
    "    24                                                 a Counter and its `update()` method is used.\n",
    "    25                                         \n",
    "    26                                             Parameters\n",
    "    27                                             ----------\n",
    "    28                                             file_path : str\n",
    "    29                                                 Path of the json file to be loaded\n",
    "    30                                         \n",
    "    31                                             Returns\n",
    "    32                                             -------\n",
    "    33                                             List[Tuple[datetime.date, str]]\n",
    "    34                                                 List of the 10 most mentioned users and\n",
    "    35                                                 the number of times they had been mentioned.\n",
    "    36                                             \"\"\"\n",
    "    37    568.6 MiB      0.0 MiB           1       module_logger.info(\"Starting Q3-TIME...\")\n",
    "    38    568.6 MiB      0.0 MiB           1       module_logger.info(\"Reading and parsing JSON file.\")\n",
    "    39    568.6 MiB      0.0 MiB           1       users_counter = Counter()\n",
    "    40    568.6 MiB      0.0 MiB           2       with open(file_path, 'r', encoding=\"utf-8\") as json_file:\n",
    "    41    568.6 MiB      0.0 MiB           1           try:\n",
    "    42    568.6 MiB      0.0 MiB      117408               for line in json_file.readlines():\n",
    "    43    568.6 MiB      0.0 MiB      117407                   data = orjson.loads(line)  # pylint: disable=E1101\n",
    "    44    568.6 MiB      0.0 MiB      117407                   mentioned_users = data.get(\"mentionedUsers\")\n",
    "    45    568.6 MiB      0.0 MiB      117407                   if mentioned_users is None:\n",
    "    46    568.6 MiB      0.0 MiB       79373                       continue\n",
    "    47    568.6 MiB      0.0 MiB      217505                   usernames = [user['username'] for user in mentioned_users]\n",
    "    48    568.6 MiB      0.0 MiB       38034                   users_counter.update(usernames)\n",
    "    49                                                 except orjson.JSONDecodeError:  # pylint: disable=E1101\n",
    "    50                                                     module_logger.error(\n",
    "    51                                                         \"There was an error reading the JSON. Some fields may be missing.\"\n",
    "    52                                                     )\n",
    "    53                                                     module_logger.info(\"The request couldn't be fulfilled.\")\n",
    "    54                                                     return []\n",
    "    55                                         \n",
    "    56    568.6 MiB      0.0 MiB           1       module_logger.info(\"Finding TOP 10 mentioned users\")\n",
    "    57                                         \n",
    "    58    568.6 MiB      0.0 MiB           1       module_logger.info(\"Ending Q3-TIME...\")\n",
    "    59                                         \n",
    "    60    568.6 MiB      0.0 MiB           1       return users_counter.most_common(10)   1       return users_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observaciones generales del Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CONSIDERACIONES INICIALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien al momento de eficientizar el tiempo de ejecuci√≥n de una funci√≥n una de las primeras cosas que vienen a la cabeza es el uso de Thread o parallelismo, en ese caso se consider√≥ que la aplicaci√≥n no deb√≠a estar pensada para trabajar con archivo mayores al que hoy se tiene (aprox. 400MB). Dada esta condici√≥n, pensar en usar Spark o Dask como frameworks de paralelismo y threading no resulta una buena idea ya que muchas veces puede terminar significando un cuello de botella. Al trabajar con archivos chicos como el de este challenge, el querer paralelizar puede terminar resultando en una aplicaci√≥n m√°s lenta. \n",
    "\n",
    "Habiendo asumido lo anterior, se decidi√≥ hacer una comparaci√≥n de tiempos para dos etapas de la funci√≥n:\n",
    "1. Etapa de Lectura\n",
    "2. Etapa de procesamiento\n",
    "\n",
    "### Etapa de lectura\n",
    "\n",
    "Se compar√≥ el m√©todo `loads()` de la librer√≠a `json` y el m√©todo `loads()` de la librer√≠a `orjson`. Esta √∫ltima librer√≠a resultaba desconocida para mi pero investigando d√≠ con ella y me result√≥ interesante poder compararlas. Al hacer la comparaci√≥n vi que `orjson` implicaba un reducci√≥n del 37% en la lectura del archivo. As√≠ que decid√≠ ir por esta.\n",
    "\n",
    "Esto se puede ver en la notebook: `notebooks\\JSON read comparison.ipynb`\n",
    "\n",
    "### Etapa de procesamiento (funciones optimizadas en tiempo)\n",
    "\n",
    "Se compar√≥ un enfoque implementado en `polars` con otro enfoque implementado en `pandas`. Si bien los tiempos eran muy similares, me result√≥ interesante plantear una soluci√≥n con `polars`. Esta librer√≠a est√° tomando mucha importancia en el √∫ltimo tiempo ya que sus m√©todos est√°n implementados de una manera m√°s eficiente dada su implementaci√≥n en `Rust`. Si bien `pandas` a√∫n sigue siendo la primera elecci√≥n y la librer√≠a con la comunidad m√°s grande, siempre es interesante evaluar alternativas. Hasta el momento no tuve la oportunidad de trabajar con `polars` en un proyecto real (con `pandas` lo hice en numerosas oportunidades) por lo que me result√≥ interesante poder implementar algo con esta librer√≠a.\n",
    "\n",
    "Esto se puede ver en la notebook: `notebooks\\Q1 processing time.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento de Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien tom√© conocimiento de la librer√≠a `emoji` decid√≠ dejar implementadas las funciones `q2_memory` y `q2_time` utilizando la RegEx. Esto se debe a que hice pruebas con la librer√≠a `emoji` (primera vez que la utilizaba)  y me encontr√© con m√©todos que trabajaban muy lento y a su vez m√©todos distintos que deber√≠a devolver resultados equivalentes pero no era as√≠. Depende del camino utilizado pod√≠a llegar a tener una cuenta distinta de emojis. \n",
    "\n",
    "Esto se puede ver en la notebook: `notebooks\\Q2 memory efficiency.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futuros pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que se quiera hacer escalar esta soluci√≥n se podr√≠a pensar en llevar la misma a un entorno Cloud como GCP, AWS o Azure.\n",
    "\n",
    "Una soluci√≥n posible para hacer Cloud esta soluci√≥n podr√≠a ser la siguiente:\n",
    "\n",
    "(Se utilizar√° **GCP** como ejemplo de referencia)\n",
    "\n",
    "1. Desplegar una imagen Docker de nuestra soluci√≥n en el servicio `Cloud Run`. De esta manera por medio de llamadas HTTP podr√≠amos ejecutar las distintas funciones que tenemos. De esta manera aprovechar√≠amos el servicio serverless, pudiendo escalar vertical y horizontalmente de acuerdo a la configuraci√≥n que nosotros asignemos y a lo que exija el sistema al querer procesar el archivo.\n",
    "\n",
    "Esto podr√≠a tener un impacto positivo en la optimizaci√≥n del tiempo de ejecuci√≥n.\n",
    "\n",
    "**Nota**: Para hacer eso previamente deberemos implementar una peque√±a API capaz de entender la requests.\n",
    "\n",
    "2. Procesar el archivo JSON previo a la ejecuci√≥n de la funciones, parse√°ndolo adecuadamente y almacen√°ndolo en servicios Cloud que permita acceder de manera r√°pida y eficiente a los datos (como a su vez procesarlos). Algunas opciones posible son BigQuery y BigQuery (si es que el volumen de datos crece significativamente). En el caso de BigQuery, podr√≠amos parsear el JSON para dejarlo de forma tabular con los datos que nos interesan y luego con consultas SQL eficientes calcular las distintas cosas que necesitamos.\n",
    "\n",
    "**Nota**: Aqu√≠ incurrimos en un costo de almacenamiento y procesamiento extra (si el volumen de datos es bajo se entre en la Free Tier).\n",
    "\n",
    "3. Podemos evaluar el uso de Dataflow para procesar el archivo\n",
    "4. Podemos evaluar armar un pipeline de transformaci√≥n con etapas de staging que permita crear checkpoints para el procesamiento de los datos, evitando as√≠ tener que reprocesar archivos enteros. Para esto podemos hacer uso de BigQuery pero principalmente pensar√≠a en Cloud Storage en primera instancia ya que nuestro input est√° en formato JSON.\n",
    "5. En caso de querer armar una App con mucha demanda podemos hacer uso del servicio Pub/Sub y as√≠ poder enviar solicitudes de procesamiento a nuestra soluci√≥n indicando, por ejemplo, de d√≥nde levantar el archvio y qu√© procesamiento aplicar.\n",
    "\n",
    "Con esto lo que quiero destacar es que la solucion aqu√≠ presentada puede crecer en gran medida. Este crecimiento vendr√° establecido por la necesidad del negocio, los objetivos del proyecto y el presupuesto del mismo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
