{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Data Engineer - LATAM - 2024\n",
    "\n",
    "Postulante: Diego Zimmerman\n",
    "\n",
    "Email: dzimmerman2611@gmail.com\n",
    "\n",
    "Empresa: Option\n",
    "\n",
    "# Objetivo del challenge\n",
    "\n",
    "    Se me proporcionó un archivo JSON (newline-delimited JSON) llamado \"farmers-protest-tweets-2021-2-4\". Este archivo contiene aproximadamente 117mil registros. Cada uno de estos registros es un objeto proveniente de la API de Twiter con información acerca del tweet.\n",
    "\n",
    "    Frente a esto se me pidió responder a tres preguntas:\n",
    "\n",
    "        1. Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días.\n",
    "        2. Los top 10 emojis más usados con su respectivo conteo.\n",
    "        3. El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos.\r\n",
    "    Para cada una de estas consignas se pidió que se implementaran dos soluciones, una enfocada en la optimización del tiempo de ejecución y la otra enfocada en la utilización de memoria.\n",
    "\n",
    "## Comentarios adicionales\n",
    "\n",
    "    Además del archivo JSONL se entregó una pequeña estructura de proyecto con algunos archivos. Entre esos archivos (los cuales se encuentran en la carpeta `src/` del proyecto) ya estaban pre-definidas las funciones que se debían completar para responder cada una de las preguntas anteriores (en sus dos versiones).\n",
    "\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo del Challenge\n",
    "\n",
    "A continuación voy a explicar cada uno de los ítems y la forma en que lo decidí resolver. Si bien esta Notebook servirá para entender el enfoque tomado, el código se encuentra documentado y las mismas justificaciones que daré a continuación se encuentran en cada una de las funciones y/o modulos implementados. Podrán existir detalles que no estén en ambos lados pero los ítems principales si lo están."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que hice fue revisar el archivo JSON para poder entender cómo estaba compuesto el mismo. Aquí presté atención a los pares clave-valor de cada JSON. \n",
    "\n",
    "Luego ingresé a la documentación oficial de la API de Twitter para contrastar lo que estaba observando con lo que decía la documentación.\n",
    "Esto lo hice para poder asegurar que la estructura de los JSONs serían constantes y poder evaluar mejor qué método de lectura sería mejor para este tipo de archivo.\n",
    "\n",
    "Luego comencé a pensar en cada función en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports y variables comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<app.logger.Logger at 0x7facd6943910>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.constants import DATA_DIR\n",
    "from src.q1_memory import q1_memory\n",
    "from src.q1_time import q1_time\n",
    "from src.q2_memory import q2_memory\n",
    "from src.q2_time import q2_time\n",
    "from src.q3_memory import q3_memory\n",
    "from src.q3_time import q3_time\n",
    "from app.main import Logger\n",
    "\n",
    "Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = DATA_DIR / \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el uso de memoria, esta función itera el archivo JSONL línea por línea. Procesar línea por línea implica procesar un JSON a la vez.\n",
    "\n",
    "El objetivo de esto es evitar cargar en memoria la totalidad del archivo y en cambio procesar cada JSON por separado. Una vez que se terminó de procesar un JSON, guardar lo que sea necesario en memoria y descartar lo demás. Recién ahí continuar con la siguiente línea del archivo (es decir, el siguiente JSON).\n",
    "\n",
    "**Explicación de función**\n",
    "\n",
    "En esta función decidí hacer una doble lectura del archivo con la intención de optimizar aún más el uso de la memoria. Para ello describiré los dos enfoques posibles:\n",
    "1. Se puede leer una única ver el archivo (leyendo línea por línea) y que a la misma vez que se está contando la cantidad de tweets por día se esté contando la cantidad de tweets que cada usuario hizo para los distintos días.\n",
    "2. Se puede leer una primera vez el archivo (leyendo línea por línea) y contar la cantidad de tweets para cada fecha. Una vez finalizada esta cuenta, encontrar los 10 días con más tweets. Una vez finalizada esta búsqueda, leeré por segunda vez el archivo (leyendo línea por línea) a fin de contar la cantidad de tweets de cada usuario. La diferencia es que ahora solo contaré los tweets de usuarios que solo hayan publicado tweets en las fechas que salieron dentro del TOP 10.\n",
    "\n",
    "Como se puede observar, el segundo enfoque permite que guarde en memoria la cuenta de tweets de usuarios que efectivamente publicaron algo en los días que salieron en el TOP 10 y no tener que almacenar la cuenta para todos los demás días que no son de interés en esta respuesta.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. Se evitaron variables intermedias y se priorizó anidar calculos a fin de evitar utilizar espacios de memorias para almacenar estas variables intermedias.\n",
    "2. Se decidió utilizar el método `nlargest` de la líbrería `heapq` frente a otros enfoques como `sorted()` dado que este método evita tener que cargar el listado completo de elementos en memoría para recién poder ordenarlos. El método utilizado permite ir guardando en memoria solo una cantidad N de elementos (en este caso 10) mientras se va iterando sobre el resto del listado (cargando un elemento a la vez en memoria).\n",
    "3. Se hace uso de `del` para eliminar variables intermedias que eran necesarias pero que después dejan de serlo (ya que su propósito fue cumplido) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_memory(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora le haremos el perfilado de tiempo y de consumo de memoria para después poder comparar con el otro enfoque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         11343289 function calls (11342043 primitive calls) in 221.810 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    17     48.5 MiB     48.5 MiB           1   @profile_function\n",
    "    18                                         @memory_profile_logging_wrapper\n",
    "    19                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    20                                             \"\"\"Answer question 1 efficiently in memory.\n",
    "    21                                         \n",
    "    22                                                 Objective: Find the 10 dates with the most tweets and\n",
    "    23                                                 the most active user for each date.\n",
    "    24                                         \n",
    "    25                                                 This function iterates over a json file and gets the TOP 10\n",
    "    26                                                 dates on which users have tweeted the most.\n",
    "    27                                         \n",
    "    28                                                 Once this is done, this function iterates over the same json looking\n",
    "    29                                                 for the most active user for each of these 10 dates.\n",
    "    30                                         \n",
    "    31                                                 We can find the users for each date in a faster way\n",
    "    32                                                 and reading the json only once, but we are interested\n",
    "    33                                                 in memory efficiency. That is why we want to avoid having a\n",
    "    34                                                 dictionary with the users' activity until we know which\n",
    "    35                                                 dates we are interested in.\n",
    "    36                                         \n",
    "    37                                             Parameters\n",
    "    38                                             ----------\n",
    "    39                                             file_path : str\n",
    "    40                                                 Path of the json file to be loaded\n",
    "    41                                         \n",
    "    42                                             Returns\n",
    "    43                                             -------\n",
    "    44                                             List[Tuple[datetime.date, str]]\n",
    "    45                                                 List of the 10 most popular dates and their users.\n",
    "    46                                                 The date and user are returned as a tuple.\n",
    "    47                                                 The list is sorted in descending order\n",
    "    48                                                 (the most tweeted date will appear first).\n",
    "    49                                             \"\"\"\n",
    "    50     48.5 MiB      0.0 MiB           1       module_logger.info(\"Starting Q1-MEMORY...\")\n",
    "    51                                         \n",
    "    52     48.5 MiB      0.0 MiB           1       module_logger.info(\"Finding TOP 10 dates\")\n",
    "    53                                             # Initialize dict that will store the number of tweets per day.\n",
    "    54     48.5 MiB      0.0 MiB           1       tweet_count_by_date = defaultdict(int)\n",
    "    55                                             # Load json line by line so memory usage is optimized\n",
    "    56     49.0 MiB      0.0 MiB           2       with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    57     49.0 MiB      0.3 MiB      117408           for line in file:\n",
    "    58     49.0 MiB      0.0 MiB      117407               tweet = json.loads(line)\n",
    "    59                                                     # Extract date and parse it\n",
    "    60                                                     # Increment tweet counter by date\n",
    "    61     49.0 MiB      0.0 MiB      352221               tweet_count_by_date[\n",
    "    62     49.0 MiB      0.3 MiB      117407                   datetime.strptime(tweet['date'].split('T')[0], DATE_FORMAT).date()\n",
    "    63     49.0 MiB      0.0 MiB      117407               ] += 1\n",
    "    64                                         \n",
    "    65                                             # Find TOP 10 dates\n",
    "    66                                             # We use heapq instead of sorted:\n",
    "    67                                             #   The heap method only stores the top n elements at any time,\n",
    "    68                                             #   whereas the sorting method requires storage of all N elements.\n",
    "    69                                             #   This means that for large datasets, the heap method's\n",
    "    70                                             #   memory usage remains constant\n",
    "    71     49.0 MiB      0.0 MiB           2       top_10_dates = heapq.nlargest(\n",
    "    72     49.0 MiB      0.0 MiB           1           10,\n",
    "    73     49.0 MiB      0.0 MiB           1           tweet_count_by_date.keys(),\n",
    "    74     49.0 MiB      0.0 MiB          27           key=lambda date: tweet_count_by_date[date],  # noqa: F821\n",
    "    75                                             )\n",
    "    76     49.0 MiB      0.0 MiB           1       del tweet_count_by_date\n",
    "    77     49.0 MiB      0.0 MiB           1       module_logger.info(\"TOP 10 dates found.\")\n",
    "    78                                         \n",
    "    79     49.0 MiB      0.0 MiB           1       module_logger.info(\"Finding most active user for each date\")\n",
    "    80                                             # Initialize dict that will store the most active user.\n",
    "    81     49.0 MiB      0.0 MiB           1       result = []\n",
    "    82     52.4 MiB      0.0 MiB          21       user_tweet_count_by_date = defaultdict(lambda: defaultdict(int))\n",
    "    83                                             # Load json line by line so memory usage is optimized\n",
    "    84     52.0 MiB      0.0 MiB           2       with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    85     52.9 MiB      0.0 MiB      117408           for line in file:\n",
    "    86     52.9 MiB      2.8 MiB      117407               tweet = json.loads(line)\n",
    "    87                                         \n",
    "    88                                                     # If date is not in `top_10_dates` skip iteration\n",
    "    89                                                     # If the top_10_dates was larger (more than 10 dates)\n",
    "    90                                                     # we should think of another data type to store these dates.\n",
    "    91                                                     # to make this `not in` clause more memory efficient.\n",
    "    92                                                     # Given that we only have 10 dates, we are going to consider\n",
    "    93                                                     # this improvement as unnecessary for now.\n",
    "    94                                                     if (\n",
    "    95     52.9 MiB      0.0 MiB      234814                   datetime.strptime(tweet['date'].split('T')[0], DATE_FORMAT).date()\n",
    "    96     52.9 MiB      0.0 MiB      117407                   not in top_10_dates\n",
    "    97                                                     ):\n",
    "    98     49.3 MiB      0.0 MiB       18040                   continue\n",
    "    99                                         \n",
    "   100     52.9 MiB      1.0 MiB      397468               user_tweet_count_by_date[\n",
    "   101     52.9 MiB      0.0 MiB       99367                   datetime.strptime(tweet['date'].split('T')[0], DATE_FORMAT).date()\n",
    "   102     52.9 MiB      0.0 MiB      198734               ][tweet[\"user\"]['username']] += 1\n",
    "   103                                         \n",
    "   104     52.9 MiB      0.0 MiB          11           for top_date in top_10_dates:\n",
    "   105                                                     # Find TOP 10 dates\n",
    "   106                                                     # We use heapq instead of sorted:\n",
    "   107                                                     #   The heap method only stores the top n elements at any time,\n",
    "   108                                                     #   whereas the sorting method requires storage of all N elements.\n",
    "   109                                                     #   This means that for large datasets, the heap method's\n",
    "   110                                                     #   memory usage remains constant\n",
    "   111                                                     # pylint: disable=W0640\n",
    "   112     52.9 MiB      0.0 MiB          30               top_user = heapq.nlargest(\n",
    "   113     52.9 MiB      0.0 MiB          10                   1,\n",
    "   114     52.9 MiB      0.0 MiB          10                   user_tweet_count_by_date[top_date].keys(),\n",
    "   115     52.9 MiB      0.0 MiB      132487                   key=lambda username: user_tweet_count_by_date[top_date][  # noqa: F821\n",
    "   116     52.9 MiB      0.0 MiB       44159                       username\n",
    "   117                                                         ],\n",
    "   118     52.9 MiB      0.0 MiB          10               )[0]\n",
    "   119                                                     # pylint: enable=W0640\n",
    "   120     52.9 MiB      0.0 MiB          10               result.append((top_date, top_user))\n",
    "   121     52.0 MiB     -0.9 MiB           1           del user_tweet_count_by_date\n",
    "   122     52.0 MiB      0.0 MiB           1       module_logger.info(\"Most active user for each date found\")\n",
    "   123     52.0 MiB      0.0 MiB           1       module_logger.info(\"Finishing Q1-MEMORY...\")\n",
    "   124     52.0 MiB      0.0 MiB           1       return resultodule_logger.info(\"Finishing Q1-MEMORY...\")\r\n",
    "   124    671.8 MiB      0.0 MiB           1       return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el tiempo, esta función busca trabajar con todo el archivo JSONL de una sola vez, evitando tener que procesar el mismo línea por línea. Si bien esto tiene un efecto significativo en memoria, es importante destacar que aquí asumí que este código siempre se ejecutará en una PC con capacidad de cómputo y memoria suficiente.\n",
    "\n",
    "**Explicación de función**\n",
    "\n",
    "Si bien la lectura línea por línea también fue necesaria aquí (dada la estructura JSONL), en esta oportunidad decidí cargar en memoria todas las líneas del archivo antes de empezar a procesar los datos.\n",
    "\n",
    "De esta manera, cargué todos los JSONs dentro del archivo en una lista que luego convertí en un DataFrame de `polars`. Una vez hecho esto realicé operaciones sencillas de agrupación (group by) y agregación (count) para llegar a la respuesta deseada.\n",
    "\r",
    "**NOTAS SOBRE EL CODIGO**\r",
    "1. a.\r\n",
    "Utilizo la librería orjson por lo que se detalla en la sección \"Observaciones generales del Challenge\" más abajo en esta notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cProfile results:\n",
    "         339557 function calls (339456 primitive calls) in 11.299 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    17     52.0 MiB     52.0 MiB           1   @profile_function\n",
    "    18                                         @memory_profile_logging_wrapper\n",
    "    19                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    20                                             \"\"\"Answer question 1 efficiently in time.\n",
    "    21                                         \n",
    "    22                                                 In this case I've used the `polars` library instead of\n",
    "    23                                                 pandas because of the results that can be seen in the\n",
    "    24                                                 `notebooks/Q1 processing time.ipynb` nootebook.\n",
    "    25                                         \n",
    "    26                                                 There is a small advantage for polars so I decided to keep\n",
    "    27                                                 polars.\n",
    "    28                                         \n",
    "    29                                                 Note: Given the size of the JSON file, there is no need for\n",
    "    30                                                 parallel processing so PySpark and Dask were discarded. These\n",
    "    31                                                 two options could had beeen an excellent solution\n",
    "    32                                                 if the size of the size happened to be bigger.\n",
    "    33                                         \n",
    "    34                                             Parameters\n",
    "    35                                             ----------\n",
    "    36                                             file_path : str\n",
    "    37                                                 Path of the json file to be loaded\n",
    "    38                                         \n",
    "    39                                             Returns\n",
    "    40                                             -------\n",
    "    41                                             List[Tuple[datetime.date, str]]\n",
    "    42                                                 List of the 10 most popular dates and their users.\n",
    "    43                                                 The date and user are returned as a tuple.\n",
    "    44                                                 The list is sorted in descending order\n",
    "    45                                                 (the most tweeted date will appear first).\n",
    "    46                                             \"\"\"\n",
    "    47     52.0 MiB      0.0 MiB           1       module_logger.info(\"Starting Q1-TIME...\")\n",
    "    48                                         \n",
    "    49     52.0 MiB      0.0 MiB           1       module_logger.info(\"Reading JSON file.\")\n",
    "    50     74.0 MiB     21.9 MiB           1       data = read_json_file(file_path)\n",
    "    51     74.0 MiB      0.0 MiB           1       module_logger.info(\"JSON file is read.\")\n",
    "    52                                         \n",
    "    53     74.0 MiB      0.0 MiB           1       module_logger.info(\"Loading DataFrame\")\n",
    "    54                                             # Create a Polars DataFrame from the processed data\n",
    "    55    100.1 MiB     26.1 MiB           2       tweets_df: pl.DataFrame = pl.DataFrame(\n",
    "    56     74.0 MiB      0.0 MiB           1           data, schema=[\"date\", \"username\"], orient=\"row\"\n",
    "    57                                             )\n",
    "    58                                         \n",
    "    59    100.1 MiB      0.0 MiB           1       module_logger.info(\"Parsing date column\")\n",
    "    60                                             # Convert the 'date' column to datetime format\n",
    "    61    154.9 MiB     54.8 MiB           2       tweets_df = tweets_df.with_columns(\n",
    "    62    100.1 MiB      0.0 MiB           1           [pl.col(\"date\").str.strptime(pl.Datetime, format=DATETIME_FORMAT, strict=False)]\n",
    "    63                                             )\n",
    "    64                                         \n",
    "    65    154.9 MiB      0.0 MiB           1       module_logger.info(\"Finding top users by date\")\n",
    "    66                                             # Group by date and username, counting the number of tweets per user each day\n",
    "    67    164.8 MiB     10.0 MiB           2       tweets_per_day = tweets_df.group_by([pl.col(\"date\").dt.date(), \"username\"]).agg(\n",
    "    68    154.9 MiB      0.0 MiB           1           pl.count().alias(\"tweet_count\")\n",
    "    69                                             )\n",
    "    70                                         \n",
    "    71                                             # Find the user with the most tweets per day\n",
    "    72    165.1 MiB      0.0 MiB           1       top_user_per_day = (\n",
    "    73    164.8 MiB      0.0 MiB           1           tweets_per_day.sort(\"tweet_count\", descending=True)\n",
    "    74    164.8 MiB      0.0 MiB           1           .group_by(\"date\")\n",
    "    75    165.1 MiB      0.3 MiB           1           .agg(pl.first(\"username\").alias(\"top_user\"))\n",
    "    76                                             )\n",
    "    77                                         \n",
    "    78    165.1 MiB      0.0 MiB           1       module_logger.info(\"Finding most tweeted dates\")\n",
    "    79                                             # Count the total number of tweets per day\n",
    "    80    165.4 MiB      0.2 MiB           2       tweets_by_day = tweets_df.group_by(tweets_df[\"date\"].dt.date()).agg(\n",
    "    81    165.1 MiB      0.0 MiB           1           pl.count().alias(\"total_tweets\")\n",
    "    82                                             )\n",
    "    83                                         \n",
    "    84    165.4 MiB      0.0 MiB           1       module_logger.info(\"Joining top users with top dates\")\n",
    "    85                                             # Join the total tweets with the top user by day\n",
    "    86    167.5 MiB      2.1 MiB           1       top_10_dates = tweets_by_day.join(top_user_per_day, on=\"date\")\n",
    "    87                                         \n",
    "    88    167.5 MiB      0.0 MiB           1       module_logger.info(\"Parsing output\")\n",
    "    89                                             # Sort by total number of tweets per day and select the top 10 dates\n",
    "    90    167.5 MiB      0.0 MiB           1       top_10_dates = top_10_dates.sort(\"total_tweets\", descending=True).head(10)\n",
    "    91                                         \n",
    "    92    167.5 MiB      0.0 MiB          24       return [\n",
    "    93    167.5 MiB      0.0 MiB          11           (row[\"date\"], row[\"top_user\"]) for row in top_10_dates.iter_rows(named=True)\n",
    "    94                                             ]tes.iter_rows(named=True)\r\n",
    "    94                                             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Los top 10 emojis más usados con su respectivo conteo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el uso de memoria, esta función itera el archivo JSONL línea por línea. Procesar línea por línea implica procesar un JSON a la vez.\n",
    "\n",
    "El objetivo de esto es evitar cargar en memoria la totalidad del archivo y en cambio procesar cada JSON por separado. Una vez que se terminó de procesar un JSON, guardar lo que sea necesario en memoria y descartar lo demás. Recién ahí continuar con la siguiente línea del archivo (es decir, el siguiente JSON).\n",
    "\n",
    "**Explicación de función**\n",
    "\n",
    "Para poder encontrar los emojis decidí hacer uso de expresiones regulares (RegEx). Para esto definí la constante `EMOJI_PATTERN` dentro del módulo `app.constants` que sirve para encontrar los emojis dentro de esos valores pre-definidos.\n",
    "\n",
    "Al cargar línea por línea el archivo, hice uso de esta constante que ya tiene definido el pattern para la expresión regular e hice uso del método `findall()`. Dado que los tweets son textos medianamente corto, el utilizar `findall()` resulta una estrategia acertada. Si el texto fuese más largo, debería evaluarse el uso de `finditer()` el cual puede ayudar a alivianar la carga en memoria en caso de que se encontrasen muchos emojis en un único tweet.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. Se evitaron variables intermedias y se priorizó anidar calculos a fin de evitar utilizar espacios de memorias para almacenar estas variables intermedias.\n",
    "2. Se decidió utilizar el método `nlargest` de la líbrería `heapq` frente a otros enfoques como `sorted()` dado que este método evita tener que cargar el listado completo de elementos en memoría para recién poder ordenarlos. El método utilizado permite ir guardando en memoria solo una cantidad N de elementos (en este caso 10) mientras se va iterando sobre el resto del listado (cargando un elemento a la vez en memoria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 1940),\n",
       " ('❤', 1397),\n",
       " ('🌾', 523),\n",
       " ('💚', 492),\n",
       " ('😂', 488),\n",
       " ('👍', 458),\n",
       " ('👉', 450),\n",
       " ('✊', 425),\n",
       " ('🇮🇳', 407),\n",
       " ('🙏🙏', 393)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         1396769 function calls (1396766 primitive calls) in 51.556 seconds.0 MiB           1       return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    16    154.6 MiB    154.6 MiB           1   @profile_function\n",
    "    17                                         @memory_profile_logging_wrapper\n",
    "    18                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    19                                             \"\"\"Answer question 2 efficiently in memory.\n",
    "    20                                         \n",
    "    21                                                 The approach used in the function is to load the JSON\n",
    "    22                                                 file line by line.\n",
    "    23                                                 If the `content' key exists, then we look for emojis in the content.\n",
    "    24                                                 Once emojis are found for that line, the dictionary with the counts is\n",
    "    25                                                 updated.\n",
    "    26                                                 After every line has been reviewed, we look for the TOP 10.\n",
    "    27                                         \n",
    "    28                                             Parameters\n",
    "    29                                             ----------\n",
    "    30                                             file_path : str\n",
    "    31                                                 Path of the json file to be loaded\n",
    "    32                                         \n",
    "    33                                             Returns\n",
    "    34                                             -------\n",
    "    35                                             List[Tuple[datetime.date, str]]\n",
    "    36                                                 List of the 10 most used emojis and\n",
    "    37                                                 the number of times they had been used.\n",
    "    38                                             \"\"\"\n",
    "    39    154.6 MiB      0.0 MiB           1       module_logger.info(\"Starting Q2-MEMORY...\")\n",
    "    40                                         \n",
    "    41    154.6 MiB      0.0 MiB           1       module_logger.info(\"Counting emojis\")\n",
    "    42    154.6 MiB      0.0 MiB           1       emoji_counter = defaultdict(int)\n",
    "    43    154.6 MiB     -1.0 MiB           2       with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    44    154.6 MiB -36855.5 MiB      117408           for line in file:\n",
    "    45    154.6 MiB -36855.5 MiB      117407               data = json.loads(line)\n",
    "    46    154.6 MiB -36855.5 MiB      117407               if 'content' not in data:\n",
    "    47                                                         continue\n",
    "    48                                                     # Extract all emojis from tweet and update dict with count\n",
    "    49    154.6 MiB -44350.6 MiB      139888               for match in EMOJI_PATTERN.findall(data[\"content\"]):\n",
    "    50    154.6 MiB  -7495.1 MiB       22481                   if match:  # Only count non-None matches\n",
    "    51    154.6 MiB  -7495.1 MiB       22481                       emoji_counter[match] += 1\n",
    "    52    153.6 MiB     -1.0 MiB           1       module_logger.info(\"Emojis counted.\")\n",
    "    53                                         \n",
    "    54    153.6 MiB      0.0 MiB           1       module_logger.info(\"Finding TOP 10 emojis\")\n",
    "    55                                             # Top 10 emojis\n",
    "    56    153.6 MiB      0.0 MiB        6227       top_10_emojis = heapq.nlargest(10, emoji_counter.items(), key=lambda x: x[1])\n",
    "    57    153.6 MiB      0.0 MiB           1       module_logger.info(\"TOP 10 emojis found.\")\n",
    "    58    153.6 MiB      0.0 MiB           1       module_logger.info(\"Finishing Q2-MEMORY...\")\n",
    "    59    153.6 MiB      0.0 MiB           1       return top_10_emojis.0 MiB           1       return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el tiempo, esta función busca trabajar con todo el archivo JSONL de una sola vez, evitando tener que procesar el mismo línea por línea. Si bien esto tiene un efecto significativo en memoria, es importante destacar que aquí asumí que este código siempre se ejecutará en una PC con capacidad de cómputo y memoria suficiente.\n",
    "\n",
    "**Explicación de función**\n",
    "\n",
    "Si bien la lectura línea por línea también fue necesaria aquí (dada la estructura JSONL), en esta oportunidad decidí cargar en memoria todas las líneas del archivo antes de empezar a procesar los datos.\n",
    "\n",
    "Como el uso de memoria no es una limitación en este caso, lo que haremos será cargar todos los tweets en un solo `string` y luego procesar este string con una Expresión Regular (RegEx) que contendrá todos los emojis que se desean encontrar.\n",
    "\n",
    "Luego de obtener la lista de emojis totales, utilizaremos un Contador para devolver los 10 primeros de estos y su correspondiente número de apariciones.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. En este caso decidi usar la clase `Counter` la cual resuelve de manera eficiente y sencilla las operaciones de cuenta.\n",
    "2. Utilizo la librería `orjson` por lo que se detalla en la sección \"Observaciones generales del Challenge\" más abajo en esta notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 1940),\n",
       " ('❤', 1397),\n",
       " ('🌾', 523),\n",
       " ('💚', 492),\n",
       " ('😂', 488),\n",
       " ('👍', 458),\n",
       " ('👉', 450),\n",
       " ('✊', 425),\n",
       " ('🇮🇳', 407),\n",
       " ('🙏🙏', 393)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         219509 function calls (219506 primitive calls) in 39.018 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    16    153.6 MiB    153.6 MiB           1   @profile_function\n",
    "    17                                         @memory_profile_logging_wrapper\n",
    "    18                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    19                                             \"\"\"Answer question 2 efficiently in time.\n",
    "    20                                         \n",
    "    21                                                 Since memory usage is not a limitation in this case,\n",
    "    22                                                 what we will do is load all the tweets into a single string\n",
    "    23                                                 and then process it with a Regular Expression (RegEx) that will contain\n",
    "    24                                                 all the emojis existing in the `emoji` library.\n",
    "    25                                         \n",
    "    26                                                 After obtaining the list of total emojis, we will use a Counter\n",
    "    27                                                 to return the top 10 of these and their corresponding\n",
    "    28                                                 number of appearances.\n",
    "    29                                         \n",
    "    30                                             Parameters\n",
    "    31                                             ----------\n",
    "    32                                             file_path : str\n",
    "    33                                                 Path of the json file to be loaded\n",
    "    34                                         \n",
    "    35                                             Returns\n",
    "    36                                             -------\n",
    "    37                                             List[Tuple[datetime.date, str]]\n",
    "    38                                                 List of the 10 most used emojis and\n",
    "    39                                                 the number of times they had been used.\n",
    "    40                                             \"\"\"\n",
    "    41    153.6 MiB      0.0 MiB           1       module_logger.info(\"Starting Q2-TIME...\")\n",
    "    42    153.6 MiB      0.0 MiB           1       module_logger.info(\"Reading and parsing JSON file.\")\n",
    "    43    153.6 MiB      0.0 MiB           1       full_text = \"\"\n",
    "    44    637.0 MiB      0.0 MiB           2       with open(file_path, 'r', encoding=\"utf-8\") as json_file:\n",
    "    45    561.2 MiB    407.6 MiB           1           tweet_list = json_file.readlines()\n",
    "    46                                         \n",
    "    47    561.2 MiB      0.0 MiB           1           try:\n",
    "    48    637.0 MiB     53.8 MiB           2               full_text = \" \".join(\n",
    "    49    583.2 MiB      4.9 MiB      234818                   [\n",
    "    50    583.2 MiB     17.0 MiB      117407                       orjson.loads(tweet)['content']  # pylint: disable=E1101\n",
    "    51    583.2 MiB      0.0 MiB      117408                       for tweet in tweet_list\n",
    "    52                                                         ]\n",
    "    53                                                     )\n",
    "    54                                                 except orjson.JSONDecodeError:  # pylint: disable=E1101\n",
    "    55                                                     module_logger.error(\n",
    "    56                                                         \"There was an error reading the JSON. Some fields may be missing.\"\n",
    "    57                                                     )\n",
    "    58                                                     module_logger.info(\"The request couldn't be fulfilled.\")\n",
    "    59                                                     return []\n",
    "    60    637.0 MiB      0.0 MiB           1       module_logger.info(\"JSON file was read and parsed.\")\n",
    "    61                                         \n",
    "    62    637.0 MiB      0.0 MiB           1       module_logger.info(\"Finding top 10 emojis\")\n",
    "    63    637.0 MiB      0.0 MiB           1       top_10_emojis = Counter(EMOJI_PATTERN.findall(full_text)).most_common(10)\n",
    "    64                                         \n",
    "    65    637.0 MiB      0.0 MiB           1       module_logger.info(\"Finishing Q2-TIME...\")\n",
    "    66    637.0 MiB      0.0 MiB           1       return top_10_emojis      0.0 MiB           1       return top_10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Enfoque en eficiencia de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el uso de memoria, esta función itera el archivo JSONL línea por línea. Procesar línea por línea implica procesar un JSON a la vez.\n",
    "\n",
    "El objetivo de esto es evitar cargar en memoria la totalidad del archivo y en cambio procesar cada JSON por separado. Una vez que se terminó de procesar un JSON, guardar lo que sea necesario en memoria y descartar lo demás. Recién ahí continuar con la siguiente línea del archivo (es decir, el siguiente JSON).\n",
    "\n",
    "**Explicación de función**\n",
    "\n",
    "Para poder encontrar los emojis decidí hacer uso de expresiones regulares (RegEx). Para esto definí la constante `EMOJI_PATTERN` dentro del módulo `app.constants` que sirve para encontrar los emojis dentro de esos valores pre-definidos.\n",
    "\n",
    "Al cargar línea por línea el archivo, hice uso de esta constante que ya tiene definido el pattern para la expresión regular e hice uso del método `findall()`. Dado que los tweets son textos medianamente corto, el utilizar `findall()` resulta una estrategia acertada. Si el texto fuese más largo, debería evaluarse el uso de `finditer()` el cual puede ayudar a alivianar la carga en memoria en caso de que se encontrasen muchos emojis en un único tweet.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. Se evitaron variables intermedias y se priorizó anidar calculos a fin de evitar utilizar espacios de memorias para almacenar estas variables intermedias.\n",
    "2. Se decidió utilizar el método `nlargest` de la líbrería `heapq` frente a otros enfoques como `sorted()` dado que este método evita tener que cargar el listado completo de elementos en memoría para recién poder ordenarlos. El método utilizado permite ir guardando en memoria solo una cantidad N de elementos (en este caso 10) mientras se va iterando sobre el resto del listado (cargando un elemento a la vez en memoria)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         1615534 function calls (1615531 primitive calls) in 66.063 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    15    570.6 MiB    570.6 MiB           1   @profile_function\n",
    "    16                                         @memory_profile_logging_wrapper\n",
    "    17                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    18                                             \"\"\"Answer question 3 efficiently in memory.\n",
    "    19                                         \n",
    "    20                                                 In this function we had two possible paths:\n",
    "    21                                                     1. Use a RegEx to find the mentions (@) in the context/text\n",
    "    22                                                       of the tweet.\n",
    "    23                                                     2. Use the `mentionedUsers` key in the JSON and leaving to\n",
    "    24                                                       the API of Twitter the identification of those mentions\n",
    "    25                                         \n",
    "    26                                                 I decided to go with the second approach.\n",
    "    27                                         \n",
    "    28                                                 This approach also include reading the JSON file line by line\n",
    "    29                                                 and counting how many times the same user is mentioned.\n",
    "    30                                                 At the end the TOP 10 users is found.\n",
    "    31                                         \n",
    "    32                                             Parameters\n",
    "    33                                             ----------\n",
    "    34                                             file_path : str\n",
    "    35                                                 Path of the json file to be loaded\n",
    "    36                                         \n",
    "    37                                             Returns\n",
    "    38                                             -------\n",
    "    39                                             List[Tuple[datetime.date, str]]\n",
    "    40                                                 List of the 10 most mentioned users and\n",
    "    41                                                 the number of times they had been mentioned.\n",
    "    42                                             \"\"\"\n",
    "    43    570.6 MiB      0.0 MiB           1       module_logger.info(\"Starting Q3-MEMORY...\")\n",
    "    44    570.6 MiB      0.0 MiB           1       users_counter = defaultdict(int)\n",
    "    45    570.6 MiB      0.0 MiB           1       module_logger.info(\"Counting mentioned users\")\n",
    "    46    570.6 MiB     -2.0 MiB           2       with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    47    570.6 MiB -177413.6 MiB      117408           for line in file:\n",
    "    48    570.6 MiB -177413.6 MiB      117407               mentioned_users = json.loads(line).get(\"mentionedUsers\")\n",
    "    49                                         \n",
    "    50    570.6 MiB -177413.6 MiB      117407               if mentioned_users is None:\n",
    "    51    570.6 MiB -118823.9 MiB       79373                   continue\n",
    "    52                                         \n",
    "    53    570.6 MiB -214651.3 MiB      141437               for user in mentioned_users:\n",
    "    54    570.6 MiB -156061.6 MiB      103403                   if user.get(\"username\") is not None:\n",
    "    55    570.6 MiB -156061.6 MiB      103403                       users_counter[user.get(\"username\")] += 1\n",
    "    56                                         \n",
    "    57    568.6 MiB     -2.0 MiB           1       module_logger.info(\"Finding TOP 10 mentioned users\")\n",
    "    58    568.6 MiB      0.0 MiB       30479       top_10_users = heapq.nlargest(10, users_counter.items(), key=lambda x: x[1])\n",
    "    59                                         \n",
    "    60    568.6 MiB      0.0 MiB           1       module_logger.info(\"Ending Q3-MEMORY...\")\n",
    "    61                                         \n",
    "    62    568.6 MiB      0.0 MiB           1       return top_10_users    0.0 MiB           1       return top_10_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Enfoque en eficiencia de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el objetivo de eficientizar el tiempo, esta función busca trabajar con todo el archivo JSONL de una sola vez, evitando tener que procesar el mismo línea por línea. Si bien esto tiene un efecto significativo en memoria, es importante destacar que aquí asumí que este código siempre se ejecutará en una PC con capacidad de cómputo y memoria suficiente.\n",
    "\n",
    "**Explicación de función**\n",
    "\n",
    "Si bien la lectura línea por línea también fue necesaria aquí (dada la estructura JSONL), en esta oportunidad decidí cargar en memoria todas las líneas del archivo antes de empezar a procesar los datos.\n",
    "\n",
    "Lo que haremos es cargar cada JSON (cada línea del JSONL) y extraer de él el campo `mentionedUsers`. Este campo es a su vez una lista de diccionarios donde cada diccionario son los datos del usuario que fue mencionado en el tweet. De esta manera, al momento de la carga de JSON en memoria, extraemos de este campo `mentionedUsers` todos los `usernames` que fueron mencionados en el tweet que estamos procesando.\n",
    "\n",
    "Una vez procesada la línea en cuestión se actualiza el objeto `Counter` y se sigue procesando la siguiente línea del archivo. De esta manera, cuando se termine de cargar el archivo se tendrá un objeto `Counter` con la cuenta que estamos buscando.\n",
    "\n",
    "Luego de obtener la lista de emojis totales, utilizaremos un Contador para devolver los 10 primeros de estos y su correspondiente número de apariciones.\n",
    "\n",
    "**NOTAS SOBRE EL CODIGO**\n",
    "1. En este caso decidi usar la clase `Counter` la cual resuelve de manera eficiente y sencilla las operaciones de cuenta.\n",
    "2. Utilizo la librería `orjson` por lo que se detalla en la sección \"Observaciones generales del Challenge\" más abajo en esta notebook.\n",
    "\n",
    "**NOTA SOBRE RESULTADO**\n",
    "Se puede observar que el enfoque para la eficiencia en memoria y en tiempo son muy parecidos, por lo cual no se observan diferencia notorias entre las dos funciones. Ambos resultados son buenos en memoria como en tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el output de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 2265),\n",
       " ('Kisanektamorcha', 1840),\n",
       " ('RakeshTikaitBKU', 1644),\n",
       " ('PMOIndia', 1427),\n",
       " ('RahulGandhi', 1146),\n",
       " ('GretaThunberg', 1048),\n",
       " ('RaviSinghKA', 1019),\n",
       " ('rihanna', 986),\n",
       " ('UNHumanRights', 962),\n",
       " ('meenaharris', 926)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de tiempo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " cProfile results:\n",
    "         565055 function calls (565052 primitive calls) in 60.092 secondss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Perfilado de memoria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deja el detalle capturado por los logs de la App (ejecutado desde conteneder `app_latam`)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "    15    568.6 MiB    568.6 MiB           1   @profile_function\n",
    "    16                                         @memory_profile_logging_wrapper\n",
    "    17                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    18                                             \"\"\"Answer question 3 efficiently in time.\n",
    "    19                                         \n",
    "    20                                                 In this function, I decided to read line by line in order\n",
    "    21                                                 to extract the mentionedUsers object from each JSON.\n",
    "    22                                         \n",
    "    23                                                 Once the mentionedUser object is retrieve a simple operation with\n",
    "    24                                                 a Counter and its `update()` method is used.\n",
    "    25                                         \n",
    "    26                                             Parameters\n",
    "    27                                             ----------\n",
    "    28                                             file_path : str\n",
    "    29                                                 Path of the json file to be loaded\n",
    "    30                                         \n",
    "    31                                             Returns\n",
    "    32                                             -------\n",
    "    33                                             List[Tuple[datetime.date, str]]\n",
    "    34                                                 List of the 10 most mentioned users and\n",
    "    35                                                 the number of times they had been mentioned.\n",
    "    36                                             \"\"\"\n",
    "    37    568.6 MiB      0.0 MiB           1       module_logger.info(\"Starting Q3-TIME...\")\n",
    "    38    568.6 MiB      0.0 MiB           1       module_logger.info(\"Reading and parsing JSON file.\")\n",
    "    39    568.6 MiB      0.0 MiB           1       users_counter = Counter()\n",
    "    40    568.6 MiB      0.0 MiB           2       with open(file_path, 'r', encoding=\"utf-8\") as json_file:\n",
    "    41    568.6 MiB      0.0 MiB           1           try:\n",
    "    42    568.6 MiB      0.0 MiB      117408               for line in json_file.readlines():\n",
    "    43    568.6 MiB      0.0 MiB      117407                   data = orjson.loads(line)  # pylint: disable=E1101\n",
    "    44    568.6 MiB      0.0 MiB      117407                   mentioned_users = data.get(\"mentionedUsers\")\n",
    "    45    568.6 MiB      0.0 MiB      117407                   if mentioned_users is None:\n",
    "    46    568.6 MiB      0.0 MiB       79373                       continue\n",
    "    47    568.6 MiB      0.0 MiB      217505                   usernames = [user['username'] for user in mentioned_users]\n",
    "    48    568.6 MiB      0.0 MiB       38034                   users_counter.update(usernames)\n",
    "    49                                                 except orjson.JSONDecodeError:  # pylint: disable=E1101\n",
    "    50                                                     module_logger.error(\n",
    "    51                                                         \"There was an error reading the JSON. Some fields may be missing.\"\n",
    "    52                                                     )\n",
    "    53                                                     module_logger.info(\"The request couldn't be fulfilled.\")\n",
    "    54                                                     return []\n",
    "    55                                         \n",
    "    56    568.6 MiB      0.0 MiB           1       module_logger.info(\"Finding TOP 10 mentioned users\")\n",
    "    57                                         \n",
    "    58    568.6 MiB      0.0 MiB           1       module_logger.info(\"Ending Q3-TIME...\")\n",
    "    59                                         \n",
    "    60    568.6 MiB      0.0 MiB           1       return users_counter.most_common(10)   1       return users_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observaciones generales del Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CONSIDERACIONES INICIALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien al momento de eficientizar el tiempo de ejecución de una función una de las primeras cosas que vienen a la cabeza es el uso de Thread o parallelismo, en ese caso se consideró que la aplicación no debía estar pensada para trabajar con archivo mayores al que hoy se tiene (aprox. 400MB). Dada esta condición, pensar en usar Spark o Dask como frameworks de paralelismo y threading no resulta una buena idea ya que muchas veces puede terminar significando un cuello de botella. Al trabajar con archivos chicos como el de este challenge, el querer paralelizar puede terminar resultando en una aplicación más lenta. \n",
    "\n",
    "Habiendo asumido lo anterior, se decidió hacer una comparación de tiempos para dos etapas de la función:\n",
    "1. Etapa de Lectura\n",
    "2. Etapa de procesamiento\n",
    "\n",
    "### Etapa de lectura\n",
    "\n",
    "Se comparó el método `loads()` de la librería `json` y el método `loads()` de la librería `orjson`. Esta última librería resultaba desconocida para mi pero investigando dí con ella y me resultó interesante poder compararlas. Al hacer la comparación vi que `orjson` implicaba un reducción del 37% en la lectura del archivo. Así que decidí ir por esta.\n",
    "\n",
    "Esto se puede ver en la notebook: `notebooks\\JSON read comparison.ipynb`\n",
    "\n",
    "### Etapa de procesamiento (funciones optimizadas en tiempo)\n",
    "\n",
    "Se comparó un enfoque implementado en `polars` con otro enfoque implementado en `pandas`. Si bien los tiempos eran muy similares, me resultó interesante plantear una solución con `polars`. Esta librería está tomando mucha importancia en el último tiempo ya que sus métodos están implementados de una manera más eficiente dada su implementación en `Rust`. Si bien `pandas` aún sigue siendo la primera elección y la librería con la comunidad más grande, siempre es interesante evaluar alternativas. Hasta el momento no tuve la oportunidad de trabajar con `polars` en un proyecto real (con `pandas` lo hice en numerosas oportunidades) por lo que me resultó interesante poder implementar algo con esta librería.\n",
    "\n",
    "Esto se puede ver en la notebook: `notebooks\\Q1 processing time.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento de Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien tomé conocimiento de la librería `emoji` decidí dejar implementadas las funciones `q2_memory` y `q2_time` utilizando la RegEx. Esto se debe a que hice pruebas con la librería `emoji` (primera vez que la utilizaba)  y me encontré con métodos que trabajaban muy lento y a su vez métodos distintos que debería devolver resultados equivalentes pero no era así. Depende del camino utilizado podía llegar a tener una cuenta distinta de emojis. \n",
    "\n",
    "Esto se puede ver en la notebook: `notebooks\\Q2 memory efficiency.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futuros pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que se quiera hacer escalar esta solución se podría pensar en llevar la misma a un entorno Cloud como GCP, AWS o Azure.\n",
    "\n",
    "Una solución posible para hacer Cloud esta solución podría ser la siguiente:\n",
    "\n",
    "(Se utilizará **GCP** como ejemplo de referencia)\n",
    "\n",
    "1. Desplegar una imagen Docker de nuestra solución en el servicio `Cloud Run`. De esta manera por medio de llamadas HTTP podríamos ejecutar las distintas funciones que tenemos. De esta manera aprovecharíamos el servicio serverless, pudiendo escalar vertical y horizontalmente de acuerdo a la configuración que nosotros asignemos y a lo que exija el sistema al querer procesar el archivo.\n",
    "\n",
    "Esto podría tener un impacto positivo en la optimización del tiempo de ejecución.\n",
    "\n",
    "**Nota**: Para hacer eso previamente deberemos implementar una pequeña API capaz de entender la requests.\n",
    "\n",
    "2. Procesar el archivo JSON previo a la ejecución de la funciones, parseándolo adecuadamente y almacenándolo en servicios Cloud que permita acceder de manera rápida y eficiente a los datos (como a su vez procesarlos). Algunas opciones posible son BigQuery y BigQuery (si es que el volumen de datos crece significativamente). En el caso de BigQuery, podríamos parsear el JSON para dejarlo de forma tabular con los datos que nos interesan y luego con consultas SQL eficientes calcular las distintas cosas que necesitamos.\n",
    "\n",
    "**Nota**: Aquí incurrimos en un costo de almacenamiento y procesamiento extra (si el volumen de datos es bajo se entre en la Free Tier).\n",
    "\n",
    "3. Podemos evaluar el uso de Dataflow para procesar el archivo\n",
    "4. Podemos evaluar armar un pipeline de transformación con etapas de staging que permita crear checkpoints para el procesamiento de los datos, evitando así tener que reprocesar archivos enteros. Para esto podemos hacer uso de BigQuery pero principalmente pensaría en Cloud Storage en primera instancia ya que nuestro input está en formato JSON.\n",
    "5. En caso de querer armar una App con mucha demanda podemos hacer uso del servicio Pub/Sub y así poder enviar solicitudes de procesamiento a nuestra solución indicando, por ejemplo, de dónde levantar el archvio y qué procesamiento aplicar.\n",
    "\n",
    "Con esto lo que quiero destacar es que la solucion aquí presentada puede crecer en gran medida. Este crecimiento vendrá establecido por la necesidad del negocio, los objetivos del proyecto y el presupuesto del mismo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
